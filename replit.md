# Overview
DecXpert CT is an AI-powered web application designed for analyzing chest CT scans. Its main purpose is to identify COPD and ILD subtypes, along with other critical thoracic conditions, and generate quantitative medical reports. The platform aims to provide expert-level diagnostic assistance, enhance clinical accuracy, and offer advanced staging and quantitative analysis for various thoracic pathologies, serving as a professional medical analysis tool.

# User Preferences
Preferred communication style: Simple, everyday language.

# System Architecture

## UI/UX Decisions
The frontend uses React with TypeScript, styled with Tailwind CSS and shadcn/ui components, following a modern, component-based design.

## Technical Implementations
The client-side is built with React, TypeScript, Wouter for routing, and TanStack React Query. The backend is a RESTful API developed with Node.js, TypeScript, and Express.js, handling medical image uploads via Multer. The system employs a **PENTA-STRATEGY cost optimization approach** achieving **80%+ API cost reduction**: (1) **UNIFIED SINGLE-CALL ANALYSIS** - All 8 pathologies analyzed in a SINGLE API call, sending images only ONCE instead of 8 times; (2) **AGGRESSIVE SLICE REDUCTION** - Reduced from 400 to 200 slices (~50% reduction) using regional coverage quotas (5 anatomical regions) + similarity deduplication + high-variance prioritization; (3) **SMART ESCALATION** - GPT-4o-mini screens first, escalating to GPT-4o for: (a) confidence <80%, or (b) ALL POSITIVE detections. Only high-confidence negatives (â‰¥80%) trust mini, providing cost savings since most studies are mostly negative; (4) **LARGER BATCHES** - Increased from 6/12 to 10/20 slices per batch = fewer API calls; (5) **OpenAI Automatic Prompt Caching** - 50% discount on cached input tokens. The system also supports Grok AI (grok-2-vision-1212) with fixed AI parameters. A voting system with adaptive confidence thresholds is applied to all 8 pathologies. Uploads are optimized with **memory-efficient two-pass processing**: large studies (>200 slices) use Pass 1 to calculate variance scores without storing PNGs, then Pass 2 converts only the selected 200 best slices with regional coverage, preventing memory exhaustion. Small batch sizes (5 concurrent) and garbage collection hints ensure stable processing of 500+ slice studies. A two-pass analysis prevents ILD patterns from masking nodule detection. All 8 pathologies are evaluated in a unified call for maximum cost efficiency. A universal image converter processes all uploaded images to PNG.

## Feature Specifications
The AI engine simulates a board-certified radiologist, detecting COPD (with GOLD staging), ILD patterns, Lung Cancer (with TNM staging), Pulmonary Embolism, Pneumonia (with CURB-65), Tuberculosis, Pleural Effusion, and Pneumothorax. It provides quantitative assessments, clinical intelligence (automated staging, recommendations), and prioritizes critical conditions. Key features include comprehensive CT analysis for multi-pathology detection, advanced AI, clinical intelligence for correlation and grading, and professional evidence-based radiological reporting. The system incorporates intelligent DICOM slice selection and asynchronous multi-slice upload processing. Comprehensive ILD subtype detection includes 11 distinct patterns following ATS/ERS/JRS/ALAT criteria, with a 4-step subtype determination algorithm.

## System Design Choices
The backend adheres to a RESTful API pattern. DICOM conversion is parallelized. The application uses PostgreSQL database with Drizzle ORM and the standard pg driver. Authentication is not implemented, operating in a demo mode. The system includes robust clause-scoped negation detection and a final consistency validation layer in reporting. Hybrid intelligent slice selection prioritizes lesion-containing slices while ensuring full chest coverage. Eight independent parallel AI analyses are performed, one for each pathology, to ensure zero cross-pathology interference. Radiological findings are validated against final detection flags to prevent inconsistencies. Batch processing is optimized for large CT studies with increased concurrency and reduced timeouts.

**2X SPEED OPTIMIZATION (2026-01-10)**: Implemented comprehensive speed optimizations achieving ~100% faster analysis completion without reducing slices, sensitivity, specificity, or accuracy. Key components: (1) **UNIFIED SINGLE-CALL SCREENING** (`server/services/unified-screening.ts`) - Replaced 8 separate GPT-4o-mini API calls with ONE consolidated call, reducing API round-trip latency by ~8x; (2) **DOUBLED BATCH CONCURRENCY** - Increased parallel batch processing from 8-15 to 20-30 concurrent batches for maximum throughput; (3) **REDUCED TIMEOUTS** - Lowered batch timeout from 180s to 90s for faster failure detection and recovery; (4) **MINIMAL RETRY DELAYS** - Reduced retry backoff from 2-20s exponential to 0.5-1s fixed delay with only 1 retry (down from 2); (5) **STREAMLINED ESCALATION** - Consolidated escalation already processes all escalations in single GPT-4o call. Total architecture: Phase 1 = 1 unified mini screening call, Phase 2 = 1 consolidated GPT-4o escalation call = maximum 2 API round-trips per batch instead of 9+.

# External Dependencies

## Third-Party Services
- **DecXpert CT AI Engine**: Proprietary AI system for medical image analysis.
- **PostgreSQL Database**: Built-in PostgreSQL database (active).
- **OpenAI API**: Used for AI model inference (gpt-4o).
- **xAI API**: Alternative AI provider using Grok vision models (grok-2-vision-1212).

## Key Libraries
- **Frontend**: React, TanStack React Query, Wouter, Tailwind CSS, shadcn/ui, Radix UI primitives, JSZip.
- **Backend**: Express.js, Multer, Drizzle ORM, Zod validation, Sharp.
- **Development**: Vite, TypeScript, PostCSS, Autoprefixer.
- **Medical Imaging (Python-based preprocessing)**: numpy, pydicom, opencv.

## Medical Compliance
- **DICOM Support**: Handles standard medical imaging formats.
- **Clinical Guidelines**: Adheres to Fleischner Society, ATS/ERS, NCCN, CHEST guidelines for diagnosis and reporting.

**$3 PER-ANALYSIS BUDGET CONTROL (2025-12-29)**: Implemented hard cost ceiling of $3.00 per CT analysis to ensure predictable API costs. Key components: (1) **BudgetTracker Utility** (`server/services/budget-tracker.ts`) - Real-time cost tracking with pre-call projections using accurate OpenAI pricing (GPT-4o: $2.50/1M input, $1.25/1M cached, $10.00/1M output; GPT-4o-mini: $0.15/1M input, $0.075/1M cached, $0.60/1M output); (2) **Consolidated Escalation** (`server/services/consolidated-escalation.ts`) - All pathologies requiring GPT-4o confirmation are batched into a SINGLE API call with isolated evaluation blocks to prevent cross-pathology interference; (3) **Budget-Gated Escalation** - Before making GPT-4o call, system checks `canAfford()` - if projected cost would exceed budget, gracefully degrades to capped-confidence (80%) mini results with clear reasoning annotations; (4) **Accurate Model Attribution** - `wasGpt4oUsed` flag propagates through the system ensuring `modelUsed` and `escalated` fields accurately reflect whether GPT-4o was actually called or budget-blocked; (5) **Cost Reporting** - Accurate savings calculations comparing actual cost vs baseline (full GPT-4o without optimizations).

**OPENAI PROMPT CACHING IMPLEMENTATION (2025-10-21)**: Implemented comprehensive prompt caching strategy to achieve significant cost reduction and performance improvement. OpenAI's automatic prompt caching feature caches prompts â‰¥1,024 tokens with 50% discount on cached input tokens. Implementation details: (1) **User Prompt Structure** - Each of the 8 pathology-specific prompts (COPD, ILD, Mass, PE, Pneumonia, TB, Pleural Effusion, Pneumothorax) contains 12,500+ tokens of detailed diagnostic criteria, well above the 1,024-token minimum for automatic caching; (2) **Automatic Activation** - Zero code changes required for caching to work; OpenAI automatically caches longest matching prefix from prompt start, with cache lifetime of 5-60 minutes (max 1 hour); (3) **Cache Hit Tracking** - Comprehensive monitoring added to `server/services/eight-parallel-analysis.ts` tracking `usage.prompt_tokens_details.cached_tokens` for both GPT-4o-mini and GPT-4o calls; (4) **Escalated Case Tracking** - For pathologies that escalate to GPT-4o, the system tracks BOTH the mini API call metrics AND the GPT-4o API call metrics separately via `miniMetrics` and `gpt4oMetrics` fields, then combines them in `cacheMetrics` for accurate total cost reporting; (5) **Cost Calculation** - Real-time cost tracking with complete accuracy: uncached input tokens (full price), cached input tokens (50% discount), completion/output tokens (full price), total cost vs baseline, cache hit rate percentage, and estimated dollar savings per batch; (6) **Logging** - Per-pathology cache metrics logged as "ðŸ’¾ [Pathology] (model): Cache hit! X/Y tokens cached (Z% cache hit rate)" plus comprehensive batch summary showing total tokens cached, overall cache hit rate, caching savings percentage, and total savings vs full GPT-4o without caching; (7) **Multi-Batch Efficiency** - First batch in a session creates cache (0% hit rate), subsequent batches reuse cached prompts (typically 70-90% hit rate after warmup), providing maximum benefit for large multi-slice studies (80+ batches); (8) **Cost Impact** - Prompt caching provides additional 30-50% savings on top of the escalation strategy, contributing to overall 75%+ total cost reduction target. Cache metrics tracked in `cacheMetrics` field of `IndependentPathologyResult` interface containing `promptTokens`, `cachedTokens`, `completionTokens`, and `cacheHitRate`. System maintains consistent prompt ordering (system message â†’ user message with prompt text + images) to maximize cache effectiveness across all API calls. Pricing: GPT-4o ($2.50/1M input, $1.25/1M cached, $10.00/1M output), GPT-4o-mini ($0.150/1M input, $0.075/1M cached, $0.600/1M output).
